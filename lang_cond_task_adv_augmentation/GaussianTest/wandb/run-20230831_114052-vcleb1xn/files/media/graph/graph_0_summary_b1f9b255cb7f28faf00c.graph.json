{"format": "torch", "nodes": [{"name": "model", "id": 140358644367184, "class_name": "MLP(\n  (layers): ModuleList(\n    (0): Linear(in_features=2, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=128, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=128, out_features=4, bias=True)\n  )\n  (network): Sequential(\n    (0): Linear(in_features=2, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=128, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=128, out_features=4, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 2]], ["layers.0.bias", [128]], ["layers.2.weight", [128, 128]], ["layers.2.bias", [128]], ["layers.4.weight", [128, 128]], ["layers.4.bias", [128]], ["layers.6.weight", [128, 128]], ["layers.6.bias", [128]], ["layers.8.weight", [4, 128]], ["layers.8.bias", [4]]], "output_shape": [[32, 4]], "num_parameters": [256, 128, 16384, 128, 16384, 128, 16384, 128, 512, 4]}, {"name": "func_model", "id": 140358634758352, "class_name": "MLP(\n  (layers): ModuleList(\n    (0): Linear(in_features=2, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=128, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=128, out_features=1, bias=True)\n  )\n  (network): Sequential(\n    (0): Linear(in_features=2, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=128, out_features=128, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=128, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=128, out_features=1, bias=True)\n  )\n)", "parameters": [["layers.0.weight", [128, 2]], ["layers.0.bias", [128]], ["layers.2.weight", [128, 128]], ["layers.2.bias", [128]], ["layers.4.weight", [128, 128]], ["layers.4.bias", [128]], ["layers.6.weight", [128, 128]], ["layers.6.bias", [128]], ["layers.8.weight", [1, 128]], ["layers.8.bias", [1]]], "output_shape": [[32, 1]], "num_parameters": [256, 128, 16384, 128, 16384, 128, 16384, 128, 128, 1]}, {"name": "loss", "id": 140358660241040, "class_name": "CrossEntropyLoss()", "parameters": [], "output_shape": [[]], "num_parameters": []}, {"name": "funv_loss", "id": 140358643954704, "class_name": "MSELoss()", "parameters": [], "output_shape": [[]], "num_parameters": []}], "edges": []}